{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42cce961",
   "metadata": {},
   "source": [
    "# Punto 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9264d7af",
   "metadata": {},
   "source": [
    "## Consigna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e512aff",
   "metadata": {},
   "source": [
    "¿Cuáles son las diferencias entre `QDA_Chol1`, `QDA_Chol2` y `QDA_Chol3`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a8f8bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "from scipy.linalg import cholesky, solve_triangular\n",
    "from scipy.linalg.lapack import dtrtri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90f4656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseBayesianClassifier:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def _estimate_a_priori(self, y):\n",
    "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
    "    # Q3: para que sirve bincount?\n",
    "    return np.log(a_priori)\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate all needed parameters for given model\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def fit(self, X, y, a_priori=None):\n",
    "    # if it's needed, estimate a priori probabilities\n",
    "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
    "\n",
    "    # now that everything else is in place, estimate all needed parameters for given model\n",
    "    self._fit_params(X, y)\n",
    "    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n",
    "\n",
    "  def predict(self, X):\n",
    "    # this is actually an individual prediction encased in a for-loop\n",
    "    m_obs = X.shape[1]\n",
    "    y_hat = np.empty(m_obs, dtype=int)\n",
    "\n",
    "    for i in range(m_obs):\n",
    "      y_hat[i] = self._predict_one(X[:,i].reshape(-1,1))\n",
    "\n",
    "    # return prediction as a row vector (matching y)\n",
    "    return y_hat.reshape(1,-1)\n",
    "\n",
    "  def _predict_one(self, x):\n",
    "    # calculate all log posteriori probabilities (actually, +C)\n",
    "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
    "                  in enumerate(self.log_a_priori) ]\n",
    "\n",
    "    # return the class that has maximum a posteriori probability\n",
    "    return np.argmax(log_posteriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "767fbc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDA_Chol1(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.L_invs = [\n",
    "        LA.inv(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True))\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L_inv = self.L_invs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = L_inv @ unbiased_x\n",
    "\n",
    "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()\n",
    "\n",
    "\n",
    "class QDA_Chol2(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.Ls = [\n",
    "        cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True)\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L = self.Ls[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = solve_triangular(L, unbiased_x, lower=True)\n",
    "\n",
    "    return -np.log(L.diagonal().prod()) -0.5 * (y**2).sum()\n",
    "\n",
    "\n",
    "class QDA_Chol3(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.L_invs = [\n",
    "        dtrtri(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True), lower=1)[0]\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L_inv = self.L_invs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = L_inv @ unbiased_x\n",
    "\n",
    "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ffdc2f",
   "metadata": {},
   "source": [
    "Las clases `QDA_Chol*` implementan, como su nombre lo indica, un algoritmo para aplicar Análisis Discriminante Cuadrático (QDA). Su diferencia está en cómo cada una realiza esta tarea. Todas se basan en la descomposición de Cholesky de la matriz de covarianianza ($\\sigma$), que consiste en encontrar una matriz triangular inferior única $L$ tal que $\\Sigma = L\\cdot L^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e1dbec",
   "metadata": {},
   "source": [
    "### QDA_Chol1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4d9d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDA_Chol1(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.L_invs = [\n",
    "        LA.inv(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True))\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L_inv = self.L_invs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = L_inv @ unbiased_x\n",
    "\n",
    "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb194bfc",
   "metadata": {},
   "source": [
    "Esta es la implementación más *naive* de todas. Su punto a favor es que resulta fácil de escribir y entender. En contra, su ineficiencia computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8651c49",
   "metadata": {},
   "source": [
    "#### _fit_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93c121e",
   "metadata": {},
   "source": [
    "Comenzamos con el ajuste. Tiene dos atributos\n",
    "\n",
    "- **Inversa:** Primero, se calcula la matriz de covarianza $\\Sigma_k$​ para los datos de la clase actual, para luego aplicarle la descomposición de Cholesky sobre $\\Sigma_k$ tal que $\\Sigma_k = L_k L_k^T$. A esto, simplemente se le aplica su inversa (lo que computacionalmente suele ser relativamente demandante).\n",
    "\n",
    "- **Vector de medias** El código itera sobre cada clase. Para cada una de ellas, selecciona de la matriz de datos $X$ solo las columnas que corresponden a esa clase. A esto, le toma la media (`axis=1` porque lo hace a través de las columnas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f54046",
   "metadata": {},
   "source": [
    "#### _predict_log_conditional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce777b80",
   "metadata": {},
   "source": [
    "A partir de `_fit_params`, obtiene vector de medias $\\mu_k$ y la inversa de Cholesky $L_k^{-1}$ para calcular $y=L_k^{-1}\\cdot (x-\\mu_k)$. El último paso es aplicar la fórmula para la log probabilidad Normal multivariante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd1a2b",
   "metadata": {},
   "source": [
    "### QDA_Chol2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1dec9",
   "metadata": {},
   "source": [
    "Acá nos evitamos directamente el cálculo de la inversa y resolvemos el sistema lineal directamente usando `solve_triangular`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1de6f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDA_Chol2(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.Ls = [\n",
    "        cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True)\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L = self.Ls[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = solve_triangular(L, unbiased_x, lower=True)\n",
    "\n",
    "    return -np.log(L.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d63ee",
   "metadata": {},
   "source": [
    "#### _fit_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc525b0b",
   "metadata": {},
   "source": [
    "Comenzamos con el ajuste. Tiene dos atributos:\n",
    "\n",
    "- **Inversa:** Calcula la matriz de covarianza $\\Sigma_k$​ para la clase actual para luego aplicar Cholesky sobre $\\Sigma_k$ para obtener la matriz triangular inferior $L_k$.\n",
    "\n",
    "- **Vector de medias** Igual a QDA_Chol1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60bcc1",
   "metadata": {},
   "source": [
    "#### _predict_log_conditional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c74b1c",
   "metadata": {},
   "source": [
    "A partir de que la ecuación $y=L_k^ {−1}​(x−\\mu_k​)$ es matemáticamente idéntica a resolver el sistema de ecuaciones lineales $L_k​y=(x−\\mu_k​)$ para la incógnita $y$, se resuelve el sistema con la función solve_triangular para luego calcular la log-probabilidad, con la diferencia de que ahora se toma el logaritmo del producto de la diagonal de la matriz L, resultando algo más directo que QDA_Chol1. Al evitar el cálculo de la inversa, esta implementación resulta la **más eficiente computacionalmente**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8195b8",
   "metadata": {},
   "source": [
    "### QDA_Chol3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72907add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDA_Chol3(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.L_invs = [\n",
    "        dtrtri(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True), lower=1)[0]\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L_inv = self.L_invs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = L_inv @ unbiased_x\n",
    "\n",
    "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ecb70d",
   "metadata": {},
   "source": [
    "Este es un punto intermedio, que no evita calcular la inversa, pero lo hace de un modo ligeramente más eficiente que QDA_Chol1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a3ad9",
   "metadata": {},
   "source": [
    "#### _fit_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a85aceb",
   "metadata": {},
   "source": [
    "Comenzamos con el ajuste. Tiene dos atributos:\n",
    "\n",
    "- **Inversa:** Calcula la matriz de covarianza $\\Sigma_k$​ para la clase actual\n",
    "para luego aplicar Cholesky sobre $\\Sigma_k$ para obtener la matriz triangular\n",
    "inferior $L_k$. Aquí, en lugar de usar la función genérica LA.inv(), utiliza\n",
    "dtrtri. Esta es una función de bajo nivel proveniente de la biblioteca LAPACK.\n",
    "Significa \"double precision triangular inverse\", y está optimizada para invertir\n",
    "matrices triangulares.\n",
    "\n",
    "- **Vector de medias** Igual a QDA_Chol1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb147066",
   "metadata": {},
   "source": [
    "#### _predict_log_conditional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4344b750",
   "metadata": {},
   "source": [
    "Idéntico a QDA_Chol1, simplemente porque ambos métodos trabajan con la inversa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
