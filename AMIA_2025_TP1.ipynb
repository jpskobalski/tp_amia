{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoTVCUtTQL6c"
   },
   "source": [
    "# TP 1: LDA/QDA y optimización matemática de modelos\n",
    "## Alumnos: \n",
    "### - Maximiliano Christener\n",
    "### - Luiz Diaz Charris\n",
    "### - Mariano Fagre\n",
    "### - Juan Pablo Skobalski\n",
    "### - Ronald Uthurralt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kL_4etdeizy"
   },
   "source": [
    "# Intro teórica\n",
    "\n",
    "## Definición: Clasificador Bayesiano\n",
    "\n",
    "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
    "\n",
    "De esta manera dicha probabilidad *a posteriori* resulta\n",
    "$$\n",
    "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
    "$$\n",
    "\n",
    "La regla de decisión de Bayes es entonces\n",
    "$$\n",
    "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G|_{X=x} = j) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_j(x) \\cdot \\pi_j \\}\n",
    "$$\n",
    "\n",
    "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
    "\n",
    "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
    "\n",
    "## Distribución condicional\n",
    "\n",
    "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
    "\n",
    "Por definición, se tiene entonces que para una clase $j$:\n",
    "$$\n",
    "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
    "$$\n",
    "\n",
    "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
    "\n",
    "## LDA\n",
    "\n",
    "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
    "$$\n",
    "\n",
    "## Entrenamiento/Ajuste\n",
    "\n",
    "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
    "\n",
    "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
    "\n",
    "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
    "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
    "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
    "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
    "\n",
    "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
    "\n",
    "## Predicción\n",
    "\n",
    "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV8OF-SlPHbD"
   },
   "source": [
    "# Código provisto\n",
    "\n",
    "Con el fin de no retrasar al alumno con cuestiones estructurales y/o secundarias al tema que se pretende tratar, se provee una base de código que **no es obligatoria de usar** pero se asume que resulta resulta beneficiosa."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PrDdJRypNB-y",
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:12.251410Z",
     "start_time": "2025-09-29T22:07:00.653015Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.linalg as LA\n",
    "from scipy.linalg import cholesky, solve_triangular\n",
    "from scipy.linalg.lapack import dtrtri"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cPL33WIN2HA"
   },
   "source": [
    "## Base code"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ewg5e0hsNTQC",
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:12.285410Z",
     "start_time": "2025-09-29T22:07:12.274863Z"
    }
   },
   "source": [
    "class BaseBayesianClassifier:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def _estimate_a_priori(self, y):\n",
    "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
    "    # Q3: para que sirve bincount?\n",
    "    # Cuenta la frecuencia de cada categoría j en y para calcular pi sub j\n",
    "    return np.log(a_priori)\n",
    "\n",
    "  def _fit_params(self, X, y): #RU: vacío, creo que está para las clases hijas\n",
    "    # estimate all needed parameters for given model\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx): #RU: vacío, creo que está para las clases hijas\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def fit(self, X, y, a_priori=None):\n",
    "    # if it's needed, estimate a priori probabilities RU: si no cargo a_priori lo calcula, si no usa el valor cargado \n",
    "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
    "\n",
    "    # now that everything else is in place, estimate all needed parameters for given model\n",
    "    self._fit_params(X, y)\n",
    "    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n",
    "    # No puede ir antes porque necesita que log_a_priori esté definida\n",
    "\n",
    "  def predict(self, X):\n",
    "    # this is actually an individual prediction encased in a for-loop\n",
    "    m_obs = X.shape[1]\n",
    "    y_hat = np.empty(m_obs, dtype=int)\n",
    "\n",
    "    for i in range(m_obs):\n",
    "      y_hat[i] = self._predict_one(X[:,i].reshape(-1,1))\n",
    "\n",
    "    # return prediction as a row vector (matching y)\n",
    "    return y_hat.reshape(1,-1)\n",
    "\n",
    "  def _predict_one(self, x):\n",
    "    # calculate all log posteriori probabilities (actually, +C)\n",
    "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i #RU:_predict_log_conditional depende del modelo \n",
    "                  in enumerate(self.log_a_priori) ]\n",
    "\n",
    "    # return the class that has maximum a posteriori probability\n",
    "    return np.argmax(log_posteriori)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rz2FC7A5NUpN",
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:12.304470Z",
     "start_time": "2025-09-29T22:07:12.298356Z"
    }
   },
   "source": [
    "class QDA(BaseBayesianClassifier):\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate each covariance matrix\n",
    "    self.inv_covs = [LA.inv(np.cov(X[:,y.flatten()==idx], bias=True)) #RU esto da la inversa de la matriz de covarianza de cada categoria\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n",
    "    # por la dimensionalidad de y. label_encode devuelve y como matriz y no como vector. Para seleccionar las columnas de X con y==idex necesitamos un vector boolano\n",
    "      \n",
    "    # Q6: por que se usa bias=True en vez del default bias=False?\n",
    "    # Porque en el cálculo de la matriz de covarianza se utiliza el estimador de MV (divide por n, no por n-1)\n",
    "      \n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "    # Q7: que hace axis=1? por que no axis=0?\n",
    "    # Porque split_transpose transpone X e y, por lo que los casos individuales están en columnas. axis=1 hace que mean calcule el promedio de los casos para cada variable en X\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    inv_cov = self.inv_covs[class_idx] #RU: class_idx toma su valor de _predict_one en _predict_log_conditional(x, idx) for...\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "    return 0.5*np.log(LA.det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x \n",
    "    #RU: 0.5*np.log(LA.det(inv_cov)) es lo mismo que -0.5 np.log(LA.det(cov))"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9lZbID0WNV1Y",
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:12.323032Z",
     "start_time": "2025-09-29T22:07:12.318103Z"
    }
   },
   "source": [
    "class TensorizedQDA(QDA):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        # ask plain QDA to fit params\n",
    "        super()._fit_params(X,y)\n",
    "\n",
    "        # stack onto new dimension\n",
    "        self.tensor_inv_cov = np.stack(self.inv_covs) #RU: apila las matrices inv_covs (k, p, p) (apila en axis=0 por default)\n",
    "        self.tensor_means = np.stack(self.means) #RU: tambien apila matrices (k, p, 1), porque self.means tiene dimensionalidad (p, 1)\n",
    "\n",
    "    def _predict_log_conditionals(self,x):\n",
    "        unbiased_x = x - self.tensor_means # np aplica broadcasting a x dimensionalidad (p, 1) y la dimensionalidad de unbiased_x es (k, p, 1)\n",
    "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
    "\n",
    "        return 0.5*np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        # return the class that has maximum a posteriori probability\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i-WGGi_sQ-pT",
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:12.343112Z",
     "start_time": "2025-09-29T22:07:12.335849Z"
    }
   },
   "source": [
    "class QDA_Chol1(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.L_invs = [\n",
    "        LA.inv(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True))\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L_inv = self.L_invs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = L_inv @ unbiased_x\n",
    "\n",
    "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i5DNLtYbQsHi",
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:14.295721Z",
     "start_time": "2025-09-29T22:07:14.287605Z"
    }
   },
   "source": [
    "class QDA_Chol2(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.Ls = [\n",
    "        cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True)\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L = self.Ls[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = solve_triangular(L, unbiased_x, lower=True)\n",
    "\n",
    "    return -np.log(L.diagonal().prod()) -0.5 * (y**2).sum()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v0dRvYVQRCgc",
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:14.399456Z",
     "start_time": "2025-09-29T22:07:14.392947Z"
    }
   },
   "source": [
    "class QDA_Chol3(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.L_invs = [\n",
    "        dtrtri(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True), lower=1)[0]\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L_inv = self.L_invs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = L_inv @ unbiased_x\n",
    "\n",
    "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCtrHQDuN6R4"
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Observar que se proveen **4 datasets diferentes**, el código de ejemplo usa uno solo pero eso no significa que ustedes se limiten al mismo. También pueden usar otros datasets de su elección."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rasInBMFNzUH",
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:14.958982Z",
     "start_time": "2025-09-29T22:07:14.420360Z"
    }
   },
   "source": [
    "from sklearn.datasets import load_iris, fetch_openml, load_wine\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_iris_dataset():\n",
    "  data = load_iris()\n",
    "  X_full = data.data\n",
    "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
    "  return X_full, y_full\n",
    "\n",
    "def get_penguins_dataset():\n",
    "    # get data\n",
    "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n",
    "\n",
    "    # drop non-numeric columns\n",
    "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
    "\n",
    "    # drop rows with missing values\n",
    "    mask = df.isna().sum(axis=1) == 0\n",
    "    df = df[mask]\n",
    "    tgt = tgt[mask]\n",
    "\n",
    "    return df.values, tgt.to_numpy().reshape(-1,1)\n",
    "\n",
    "def get_wine_dataset():\n",
    "    # get data\n",
    "    data = load_wine()\n",
    "    X_full = data.data\n",
    "    y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
    "    return X_full, y_full\n",
    "\n",
    "def get_letters_dataset():\n",
    "    # get data\n",
    "    letter = fetch_openml('letter', version=1, as_frame=False)\n",
    "    return letter.data, letter.target.reshape(-1,1)\n",
    "\n",
    "def label_encode(y_full):\n",
    "    return LabelEncoder().fit_transform(y_full.flatten()).reshape(y_full.shape)\n",
    "\n",
    "def split_transpose(X, y, test_size, random_state):\n",
    "    # X_train, X_test, y_train, y_test but all transposed\n",
    "    return [elem.T for elem in train_test_split(X, y, test_size=test_size, random_state=random_state)]"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdatasets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m load_iris, fetch_openml, load_wine\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpreprocessing\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LabelEncoder\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodel_selection\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'sklearn'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybPkuBdDN42P"
   },
   "source": [
    "## Benchmarking\n",
    "\n",
    "Nota: esta clase fue creada bastante rápido y no pretende ser una plataforma súper confiable sobre la que basarse, sino más bien una herramienta simple con la que poder medir varios runs y agregar la información.\n",
    "\n",
    "En forma rápida, `warmup` es la cantidad de runs para warmup, `mem_runs` es la cantidad de runs en las que se mide el pico de uso de RAM y `n_runs` es la cantidad de runs en las que se miden tiempos.\n",
    "\n",
    "La razón por la que se separan es que medir memoria hace ~2.5x más lento cada run, pero al mismo tiempo se estabiliza mucho más rápido.\n",
    "\n",
    "**Importante:** tener en cuenta que los modelos que predicen en batch (usan `predict` directamente) deberían consumir, como mínimo, $n$ veces la memoria de los que predicen por observación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:15.134521300Z",
     "start_time": "2025-09-22T12:26:10.655346Z"
    },
    "id": "nO4Py3CeNpKu"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from numpy.random import RandomState\n",
    "import tracemalloc\n",
    "\n",
    "RNG_SEED = 6553\n",
    "\n",
    "class Benchmark:\n",
    "    def __init__(self, X, y, n_runs=1000, warmup=100, mem_runs=100, test_sz=0.3, rng_seed=RNG_SEED, same_splits=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n = n_runs\n",
    "        self.warmup = warmup\n",
    "        self.mem_runs = mem_runs\n",
    "        self.test_sz = test_sz\n",
    "        self.det = same_splits\n",
    "        if self.det:\n",
    "            self.rng_seed = rng_seed\n",
    "        else:\n",
    "            self.rng = RandomState(rng_seed)\n",
    "\n",
    "        self.data = dict()\n",
    "\n",
    "        print(\"Benching params:\")\n",
    "        print(\"Total runs:\",self.warmup+self.mem_runs+self.n)\n",
    "        print(\"Warmup runs:\",self.warmup)\n",
    "        print(\"Peak Memory usage runs:\", self.mem_runs)\n",
    "        print(\"Running time runs:\", self.n)\n",
    "        approx_test_sz = int(self.y.size * self.test_sz)\n",
    "        print(\"Train size rows (approx):\",self.y.size - approx_test_sz)\n",
    "        print(\"Test size rows (approx):\",approx_test_sz)\n",
    "        print(\"Test size fraction:\",self.test_sz)\n",
    "\n",
    "    def bench(self, model_class, **kwargs):\n",
    "        name = model_class.__name__\n",
    "        time_data = np.empty((self.n, 3), dtype=float)  # train_time, test_time, accuracy\n",
    "        mem_data = np.empty((self.mem_runs, 2), dtype=float)  # train_peak_mem, test_peak_mem\n",
    "        rng = RandomState(self.rng_seed) if self.det else self.rng\n",
    "\n",
    "\n",
    "        for i in range(self.warmup):\n",
    "            # Instantiate model with error check for unsupported parameters\n",
    "            model = model_class(**kwargs)\n",
    "\n",
    "            # Generate current train-test split\n",
    "            X_train, X_test, y_train, y_test = split_transpose(\n",
    "                self.X, self.y,\n",
    "                test_size=self.test_sz,\n",
    "                random_state=rng\n",
    "            )\n",
    "            # Run training and prediction (timing or memory measurement not recorded)\n",
    "            model.fit(X_train, y_train)\n",
    "            model.predict(X_test)\n",
    "\n",
    "        for i in tqdm(range(self.mem_runs), total=self.mem_runs, desc=f\"{name} (MEM)\"):\n",
    "\n",
    "            model = model_class(**kwargs)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = split_transpose(\n",
    "                self.X, self.y,\n",
    "                test_size=self.test_sz,\n",
    "                random_state=rng\n",
    "            )\n",
    "\n",
    "            tracemalloc.start()\n",
    "\n",
    "            t1 = time.perf_counter()\n",
    "            model.fit(X_train, y_train)\n",
    "            t2 = time.perf_counter()\n",
    "\n",
    "            _, train_peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.reset_peak()\n",
    "\n",
    "            model.predict(X_test)\n",
    "            t3 = time.perf_counter()\n",
    "            _, test_peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "\n",
    "            mem_data[i,] = (\n",
    "                train_peak / (1024 * 1024),\n",
    "                test_peak / (1024 * 1024)\n",
    "            )\n",
    "\n",
    "        for i in tqdm(range(self.n), total=self.n, desc=f\"{name} (TIME)\"):\n",
    "            model = model_class(**kwargs)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = split_transpose(\n",
    "                self.X, self.y,\n",
    "                test_size=self.test_sz,\n",
    "                random_state=rng\n",
    "            )\n",
    "\n",
    "            t1 = time.perf_counter()\n",
    "            model.fit(X_train, y_train)\n",
    "            t2 = time.perf_counter()\n",
    "            preds = model.predict(X_test)\n",
    "            t3 = time.perf_counter()\n",
    "\n",
    "            time_data[i,] = (\n",
    "                (t2 - t1) * 1000,\n",
    "                (t3 - t2) * 1000,\n",
    "                (y_test.flatten() == preds.flatten()).mean()\n",
    "            )\n",
    "\n",
    "        self.data[name] = (time_data, mem_data)\n",
    "\n",
    "    def summary(self, baseline=None):\n",
    "        aux = []\n",
    "        for name, (time_data, mem_data) in self.data.items():\n",
    "            result = {\n",
    "                'model': name,\n",
    "                'train_median_ms': np.median(time_data[:, 0]),\n",
    "                'train_std_ms': time_data[:, 0].std(),\n",
    "                'test_median_ms': np.median(time_data[:, 1]),\n",
    "                'test_std_ms': time_data[:, 1].std(),\n",
    "                'mean_accuracy': time_data[:, 2].mean(),\n",
    "                'train_mem_median_mb': np.median(mem_data[:, 0]),\n",
    "                'train_mem_std_mb': mem_data[:, 0].std(),\n",
    "                'test_mem_median_mb': np.median(mem_data[:, 1]),\n",
    "                'test_mem_std_mb': mem_data[:, 1].std()\n",
    "            }\n",
    "            aux.append(result)\n",
    "        df = pd.DataFrame(aux).set_index('model')\n",
    "\n",
    "        if baseline is not None and baseline in self.data:\n",
    "            df['train_speedup'] = df.loc[baseline, 'train_median_ms'] / df['train_median_ms']\n",
    "            df['test_speedup'] = df.loc[baseline, 'test_median_ms'] / df['test_median_ms']\n",
    "            df['train_mem_reduction'] = df.loc[baseline, 'train_mem_median_mb'] / df['train_mem_median_mb']\n",
    "            df['test_mem_reduction'] = df.loc[baseline, 'test_mem_median_mb'] / df['test_mem_median_mb']\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb5VEpEugFXW"
   },
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:15.136521700Z",
     "start_time": "2025-09-22T12:26:37.832008Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLyr4-hdgJ7e",
    "outputId": "86428138-3982-4c05-8a88-f6809d524a27"
   },
   "outputs": [],
   "source": [
    "# levantamos el dataset Wine, que tiene 13 features y 178 observaciones en total\n",
    "X_full, y_full = get_wine_dataset()\n",
    "X_full.shape, y_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:15.136521700Z",
     "start_time": "2025-09-22T12:26:39.525017Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZxQlFUSbgYHQ",
    "outputId": "32c0fee8-2a79-4f8c-c526-392f026fff1e"
   },
   "outputs": [],
   "source": [
    "# encodeamos a número las clases\n",
    "y_full_encoded = label_encode(y_full)\n",
    "\n",
    "y_full[:5], y_full_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:15.136521700Z",
     "start_time": "2025-09-22T12:26:40.902492Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSBNNUOmgtsI",
    "outputId": "7dc60ba1-0548-4671-8f44-a9fb227360d7"
   },
   "outputs": [],
   "source": [
    "# generamos el benchmark\n",
    "# observar que son valores muy bajos de runs para que corra rápido ahora\n",
    "b = Benchmark(\n",
    "    X_full, y_full_encoded,\n",
    "    n_runs = 100,\n",
    "    warmup = 20,\n",
    "    mem_runs = 20,\n",
    "    test_sz = 0.3,\n",
    "    same_splits = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:15.136521700Z",
     "start_time": "2025-09-22T12:26:42.930341Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "c01ec4015f1c4c4491687244977cf5ee",
      "f07c3a390d8640e586ec729d3c7c86d8",
      "1d3c4fad476c47208bce6625ec783f6d",
      "a33f440ec8454a2db5bd26485d71c4db",
      "766927c256e3409ab53c04ca092964e0",
      "54a06911978746dd9e81bb1526bd741c",
      "a806c90e849243999c2fce804e20b449",
      "0c4640fad3de46af8ee288fb94e9831d",
      "70c3a115637d4880802a605974661714",
      "3f110d0a165e41b0a268f77b2d2a11b7",
      "de398ad23ecd44d8b8d69e621401f3b3",
      "d0c5a2b6b27144268b7fc0bedcc33a86",
      "684de1d1bee34b309988bc0f40b0d5e2",
      "3a99fe4554914c12ad9146aea349708c",
      "1da7d96834cc4bc8a3cd5595b3446b74",
      "d21ef446c090431480050c3352e2634e",
      "d3fe060bd70848b7af130d99bbff6839",
      "32df4fc69a3c47cf9a220f4469b667a3",
      "254e2937d92d48b0886e5d950a6bbefa",
      "0cead8148ae5469eb637c29de2b21ec7",
      "de9f39ade1b04c22921589c687b25518",
      "437e4360cf5e44bc845c2f6002ea2d55"
     ]
    },
    "id": "zUciOjazhUu5",
    "outputId": "398afdb9-ef70-40db-b771-eb3f883a68f0"
   },
   "outputs": [],
   "source": [
    "# bencheamos un par\n",
    "to_bench = [QDA_Chol3]\n",
    "\n",
    "for model in to_bench:\n",
    "    b.bench(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:15.136521700Z",
     "start_time": "2025-09-22T12:26:44.475491Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "8aeeac18744940f8b4940256613178db",
      "16f5ac0cf3cc441d891ae9522cf0798c",
      "bc84ff51ad3c4921b7aca214f0ff5e35",
      "f8508a02a66547b3a501694a1b8633ab",
      "4ab824715b294c86b045c7e187125524",
      "5e4f83990b0c4572952d5e23005283fb",
      "b85cd793196847cda92178f945624016",
      "b1c06f1940204bdaacf4ad3d843faa2f",
      "74c98c4598bd438cbedbb6c8a492eb15",
      "373a7a1549b44c2696083efddc88fafe",
      "8e2c2ff9e34b49568449aa7eb991d5cb",
      "de1229493a5749c799bcece8c07506f5",
      "c29e6bbacf9c45ecb5465624ee315b70",
      "a2e9076aca0b4224ba3aa8e4c3239260",
      "8df46d45d37e45e889dbec220fa360bd",
      "514491be27294e0cb5b70b6e25ee3355",
      "6d70e647d03f4004bd727673fbbfe5f9",
      "c3149d7e47c84328904326dda8527af4",
      "6197d697782b4688a278663e287317b1",
      "c69d75dc1d1b4fca925f308a79a7e248",
      "7f722aef90b84352b16d45f8716df168",
      "f356ef11b0734ce6a31ec9f0a01e055a"
     ]
    },
    "id": "wpPhSSCNhlvG",
    "outputId": "3edd56df-f71f-41bd-9f02-caa837541aa2"
   },
   "outputs": [],
   "source": [
    "# como es una clase, podemos seguir bencheando más después\n",
    "b.bench(TensorizedQDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:15.136521700Z",
     "start_time": "2025-09-22T12:26:53.717838Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "bZ5-vowshr5c",
    "outputId": "f494db5c-f2a5-46ba-a718-b7ba68f0699d"
   },
   "outputs": [],
   "source": [
    "# hacemos un summary\n",
    "b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:15.137521400Z",
     "start_time": "2025-09-22T12:26:56.326660Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "09eKXqlXhwL-",
    "outputId": "a721b3e9-9003-4d2c-9f7e-11a96b0870d6"
   },
   "outputs": [],
   "source": [
    "# son muchos datos! nos quedamos con un par nomás\n",
    "summ = b.summary()\n",
    "\n",
    "# como es un pandas DataFrame, subseteamos columnas fácil\n",
    "summ[['train_median_ms', 'test_median_ms','mean_accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:15.137521400Z",
     "start_time": "2025-09-22T12:26:58.425003Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "EopB9574h8I5",
    "outputId": "b8bf48a0-f791-4a34-da7c-254071cbadf7"
   },
   "outputs": [],
   "source": [
    "# podemos setear un baseline para que fabrique columnas de comparación\n",
    "summ = b.summary(baseline='QDA')\n",
    "\n",
    "summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:15.137521400Z",
     "start_time": "2025-09-22T12:27:00.685472Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "z0qeE1gviFLZ",
    "outputId": "760de925-f8fc-4e77-a8ce-50785d14e487"
   },
   "outputs": [],
   "source": [
    "summ[[\n",
    "    'train_median_ms', 'test_median_ms','mean_accuracy',\n",
    "    'train_speedup', 'test_speedup',\n",
    "    'train_mem_reduction', 'test_mem_reduction',\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EF80Pck2RmaC"
   },
   "source": [
    "# Consigna QDA\n",
    "\n",
    "**Notación**: en general notamos\n",
    "\n",
    "* $k$ la cantidad de clases\n",
    "* $n$ la cantidad de observaciones\n",
    "* $p$ la cantidad de features/variables/predictores\n",
    "\n",
    "**Sugerencia:** combinaciones adecuadas de `transpose`, `stack`, `reshape` y, ocasionalmente, `flatten` y `diagonal` suele ser más que suficiente. Se recomienda *fuertemente* explorar la dimensionalidad de cada elemento antes de implementar las clases.\n",
    "\n",
    "## Tensorización\n",
    "\n",
    "En esta sección nos vamos a ocupar de hacer que el modelo sea más rápido para generar predicciones, observando que incurre en un doble `for` dado que predice en forma individual un escalar para cada observación, para cada clase. Paralelizar ambos vía tensorización suena como una gran vía de mejora de tiempos.\n",
    "\n",
    "### 1) Diferencias entre `QDA`y `TensorizedQDA`\n",
    "\n",
    "1. ¿Sobre qué paraleliza `TensorizedQDA`? ¿Sobre las $k$ clases, las $n$ observaciones a predecir, o ambas?\n",
    "2. Analizar los shapes de `tensor_inv_covs` y `tensor_means` y explicar paso a paso cómo es que `TensorizedQDA` llega a predecir lo mismo que `QDA`.\n",
    "\n",
    "### 2) Optimización\n",
    "\n",
    "Debido a la forma cuadrática de QDA, no se puede predecir para $n$ observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de $n \\times n$ en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
    "\n",
    "3. Implementar el modelo `FasterQDA` (se recomienda heredarlo de `TensorizedQDA`) de manera de eliminar el ciclo for en el método predict.\n",
    "4. Mostrar dónde aparece la mencionada matriz de $n \\times n$, donde $n$ es la cantidad de observaciones a predecir.\n",
    "5. Demostrar que\n",
    "$$\n",
    "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
    "$$ es decir, que se puede \"esquivar\" la matriz de $n \\times n$ usando matrices de $n \\times p$. También se puede usar, de forma equivalente,\n",
    "$$\n",
    "np.sum(A^T \\odot B, axis=0).T\n",
    "$$\n",
    "queda a preferencia del alumno cuál usar.\n",
    "6. Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente en un nuevo modelo `EfficientQDA`.\n",
    "7. Comparar la performance de las 4 variantes de QDA implementadas hasta ahora (no Cholesky) ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?\n",
    "\n",
    "## Cholesky\n",
    "\n",
    "Hasta ahora todos los esfuerzos fueron enfocados en realizar una predicción más rápida. Los tiempos de entrenamiento (teóricos al menos) siguen siendo los mismos o hasta (minúsculamente) peores, dado que todas las mejoras siguen llamando al método `_fit_params` original de `QDA`.\n",
    "\n",
    "La descomposición/factorización de [Cholesky](https://en.wikipedia.org/wiki/Cholesky_decomposition#Statement) permite factorizar una matriz definida positiva $A = LL^T$ donde $L$ es una matriz triangular inferior. En particular, si bien se asume que $p \\ll n$, invertir la matriz de covarianzas $\\Sigma$ para cada clase impone un cuello de botella que podría alivianarse. Teniendo en cuenta que las matrices de covarianza son simétricas y salvo degeneración, definidas positivas, Cholesky como mínimo debería permitir invertir la matriz más rápido.\n",
    "\n",
    "*Nota: observar que calcular* $A^{-1}b$ *equivale a resolver el sistema* $Ax=b$.\n",
    "\n",
    "### 3) Diferencias entre implementaciones de `QDA_Chol`\n",
    "\n",
    "8. Si una matriz $A$ tiene fact. de Cholesky $A=LL^T$, expresar $A^{-1}$ en términos de $L$. ¿Cómo podría esto ser útil en la forma cuadrática de QDA?\n",
    "9. Explicar las diferencias entre `QDA_Chol1`y `QDA` y cómo `QDA_Chol1` llega, paso a paso, hasta las predicciones.\n",
    "10. ¿Cuáles son las diferencias entre `QDA_Chol1`, `QDA_Chol2` y `QDA_Chol3`?\n",
    "11. Comparar la performance de las 7 variantes de QDA implementadas hasta ahora ¿Qué se observa?¿Hay alguna de las implementaciones de `QDA_Chol` que sea claramente mejor que las demás?¿Alguna que sea peor?\n",
    "\n",
    "### 4) Optimización\n",
    "\n",
    "12. Implementar el modelo `TensorizedChol` paralelizando sobre clases/observaciones según corresponda. Se recomienda heredarlo de alguna de las implementaciones de `QDA_Chol`, aunque la elección de cuál de ellas queda a cargo del alumno según lo observado en los benchmarks de puntos anteriores.\n",
    "13. Implementar el modelo `EfficientChol` combinando los insights de `EfficientQDA` y `TensorizedChol`. Si se desea, se puede implementar `FasterChol` como ayuda, pero no se contempla para el punto.\n",
    "14. Comparar la performance de las 9 variantes de QDA implementadas ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcqVvRSLwaEZ"
   },
   "source": [
    "## Importante:\n",
    "\n",
    "Las métricas que se observan al realizar benchmarking son muy dependientes del código que se ejecuta, y por tanto de las versiones de las librerías utilizadas. Una forma de unificar esto es utilizando un gestor de versiones y paquetes como _uv_ o _Poetry_, otra es simplemente usando una misma VM como la que provee Colab.\n",
    "\n",
    "**Cada equipo debe informar las versiones de Python, NumPy y SciPy con que fueron obtenidos los resultados. En caso de que sean múltiples, agregar todos los casos**. La siguiente celda provee una ayuda para hacerlo desde un notebook, aunque como es una secuencia de comandos también sirve para consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:15.138523500Z",
     "start_time": "2025-09-23T16:31:32.385947Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwlW7sqwirdn",
    "outputId": "ccccb229-089a-44fb-cda3-da0a11b4fd4f"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 --version\n",
    "pip freeze | grep -E \"scipy|numpy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNWRw6ofUNgZ"
   },
   "source": [
    "**Comentario:** yo utilicé los siguientes parámetros para mi run de prueba. Esto NO significa que ustedes tengan que usar los mismos, tampoco el mismo dataset. Se agregó al notebook simplemente porque fue una pregunta común en cohortes anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:15.138523500Z",
     "start_time": "2025-09-22T15:58:34.092615Z"
    },
    "id": "9Vn-q4RJv8aA"
   },
   "outputs": [],
   "source": [
    "# dataset de letters\n",
    "X_letter, y_letter = get_letters_dataset()\n",
    "\n",
    "# encoding de labels\n",
    "y_letter_encoded = label_encode(y_letter.reshape(-1,1))\n",
    "\n",
    "# instanciacion del benchmark\n",
    "b = Benchmark(\n",
    "    X_letter, y_letter_encoded,\n",
    "    same_splits=False,\n",
    "    n_runs=100,\n",
    "    warmup=20,\n",
    "    mem_runs=30,\n",
    "    test_sz=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:15.138523500Z",
     "start_time": "2025-09-22T21:08:55.445117Z"
    }
   },
   "outputs": [],
   "source": [
    "%%cmd\n",
    "python --version\n",
    "pip freeze | findstr \"scipy numpy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1) Diferencias entre `QDA`y `TensorizedQDA`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Sobre qué paraleliza `TensorizedQDA`? ¿Sobre las $k$ clases, las $n$ observaciones a predecir, o ambas?\n",
    "\n",
    "TensorizedQDA paraleliza sobre k (clases) y no paraleliza sobre n (observaciones). En BayesClassifier `_predict_one(self, x)` tiene un loop que analiza todas las clases para una observación `log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i in enumerate(self.log_a_priori) ]`\n",
    "   En BayesClassifier `predict(self, X)` tiene otro loop que corre `_predict(self, X)` para cada una de las observaciones `for i in range(m_obs): y_hat[i] = self._predict_one(X[:,i].reshape(-1,1))`. QDA recurre a estos dos bucles para calcular la predicción, usando `_predict_log_conditional(self, x, class_idx)` para cada combinación de `x` y de `class_idx`.\n",
    "   TensorizedQDA calcula `_predict_log_conditionals(self,x):` en forma tensorizada y no recurre al bucle que calcula la predicción para cada `class_idx`. Luego corre el bucle `for i in range(m_obs): y_hat[i] = self._predict_one(X[:,i].reshape(-1,1))` de `predict(self, X)` en BayesClassifier.\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "2. Analizar los shapes de `tensor_inv_covs` y `tensor_means` y explicar paso a paso cómo es que `TensorizedQDA` llega a predecir lo mismo que `QDA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.tensor_inv_cov = np.stack(self.inv_covs) \n",
    "        # np.stack(self.inv_covs) apila las k matrices (p x p) de self.inv_covs en un tensor (k x p x p) (apila k veces en axis=0 por default)\n",
    "        self.tensor_means = np.stack(self.means)\n",
    "        # np.stack(self.means) apila las k matrices (p x 1) de self.means en un tensor (k x p x 1)\n",
    "\n",
    "    def _predict_log_conditionals(self,x):\n",
    "        unbiased_x = x - self.tensor_means\n",
    "        # np aplica broadcasting a x dimensionalidad (p x 1) y lo expande a (k x p x 1) para que coincida con \n",
    "        # la dimensionalidad de tensor_means (k x p x 1)\n",
    "        # la resta de x (k x p x 1) menos tensor_means (k x p x 1) mantiene la dimensionalidad para unbiased_x (k x p x 1)\n",
    "        \n",
    "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
    "        # la trasposición(0,2,1) de unbiased_x da un tensor con dimensionalidad (k x 1 x p)\n",
    "        # self.tensor_inv_covs tenía dimensionalidad (k x p x p) y unbiased_x (k x p x 1)\n",
    "        # Si el primer eje concide en dimensionalidad (r), @ de dos tensores multiplica sobre los dos últimos ejes (m x n) y (n x q)\n",
    "        # para cada r y los apila en un tensor con dimensionalidad (r x m x q)\n",
    "        # en nuestro caso unbiased_x.transpose(0,2,1) (k x 1 x p) @ self.tensor_inv_cov (k x p x p) resulta en un tensor (k x 1 x p)\n",
    "        # y este @ unbiased_x (k x p x 1) resulta en un tensor (k x 1 x 1)\n",
    "        # pero esto es lo mismo que hacer para cada k (x-self.means).T @ self.inv_cons @ (x-self.means), tal como se hace en QDA con un bucle\n",
    "        \n",
    "        return 0.5*np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
    "        # flatten() transforma a inner_prod (k x 1 x 1) en un vector (k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2) Optimizacion\n",
    "\n",
    "3. Implementar el modelo `FasterQDA` (se recomienda heredarlo de `TensorizedQDA`) de manera de eliminar el ciclo for en el método predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T22:07:15.139033800Z",
     "start_time": "2025-09-22T16:04:21.600793Z"
    }
   },
   "outputs": [],
   "source": [
    "class FasterQDA(TensorizedQDA):\n",
    "    \"\"\"\n",
    "    Vectorizamos la prediccion sobre TODAS las observaciones a la vez. Esta versión arma una matriz (k, n, n) por clase y despues soquedamos unicamente con la diagonal. \n",
    "    \"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: array de shape (p, n)  -> p features, n observaciones\n",
    "        return: array de shape (n,) con la clase predicha por observación\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Centramos todas las observaciones X con el vector de medias de CADA clase\n",
    "        #    - self.tensor_means tiene shape (k, p, 1)\n",
    "        #    - X tiene shape (p, n)\n",
    "        #    Con broadcasting, obtengo unbiased de shape (k, p, n): para cada clase k, resto su media a todas las columnas de X.\n",
    "        unbiased = X[None, :, :] - self.tensor_means  # (k, p, n)\n",
    "\n",
    "        # 2) Aplicamos la forma cuadratica (x - μ)^T Σ^{-1} (x - μ) pero en batches de observaciones:\n",
    "        #    Primero transponemos unbiased para tener (k, n, p) a la izquierda, y dejamos (k, p, p) y (k, p, n) en el medio y derecha.\n",
    "        #    El producto da M con shape (k, n, n): ahi aparecen TODAS las interacciones entre observaciones.\n",
    "        m = np.transpose(unbiased, (0, 2, 1)) @ self.tensor_inv_cov @ unbiased  # (k, n, n)\n",
    "\n",
    "        # 3) Para la forma cuadratica solo necesitamps la diagonal por clase/obs:\n",
    "        #    diag(M) tiene shape (k, n). Nos ahorramos usar toda la matriz M despues.\n",
    "        inner = np.diagonal(m, axis1=1, axis2=2)  # (k, n)\n",
    "\n",
    "        # 4) terminamos log-determinante para cada clase:\n",
    "        #    uso slogdet por estabilidad numérica.\n",
    "        #    Ojo: tengo Σ^{-1} ya precalculada; log|Σ^{-1}| = -log|Σ|.\n",
    "        #    slogdet sobre (k, p, p) me devuelve arrays de largo k.\n",
    "        sign, logdet_inv = LA.slogdet(self.tensor_inv_cov)  # (k,)\n",
    "\n",
    "        # 5) armamos los log-condicionales por clase y por observacion:\n",
    "        #    broadcasting para que el logdet_inv (k,) se expanda a (k, n).\n",
    "        log_conditionals = 0.5 * logdet_inv[:, None] - 0.5 * inner  # (k, n)\n",
    "\n",
    "        # 6) sumamos log-priors por clase y elejimos la clase con mayor puntaje para cada obs\n",
    "        scores = self.log_a_priori[:, None] + log_conditionals  # (k, n)\n",
    "        yhat = np.argmax(scores, axis=0)  # (n,)\n",
    "        return yhat\n",
    "\n",
    "    # Dejo un helper opcional si quiero reusar la parte de log-condicionales en otros métodos\n",
    "    def _predict_log_conditionals_batch(self, X):\n",
    "        \"\"\"\n",
    "        Devolvemos la matriz (k, n) con los log-condicionales para todas las observaciones.\n",
    "        Solo lo usamos si queremos calcular proba/scores sin repetir codigo.\n",
    "        \"\"\"\n",
    "        unbiased = X[None, :, :] - self.tensor_means # (k, p, n)\n",
    "        m = np.transpose(unbiased, (0, 2, 1)) @ self.tensor_inv_cov @ unbiased # (k, n, n)\n",
    "        inner = np.diagonal(m, axis1=1, axis2=2) # (k, n)\n",
    "        _, logdet_inv = LA.slogdet(self.tensor_inv_cov) # (k,)\n",
    "        return 0.5 * logdet_inv[:, None] - 0.5 * inner # (k, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Mostrar dónde aparece la mencionada matriz de $n \\times n$, donde $n$ es la cantidad de observaciones a predecir.**\n",
    "\n",
    "La matriz de tamaño $n \\times n$ aparece en el paso de vectorizacion de la prediccion, cuando calculamos en bloque la forma cuadratica para todas las observaciones y todas las clases. En el metodo `predict` de `FasterQDA` se construye:\n",
    "\n",
    "```python\n",
    "m = np.transpose(unbiased, (0, 2, 1)) @ self.tensor_inv_cov @ unbiased  # (k, n, n)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Demostrar que**\n",
    "$$\n",
    "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
    "$$ es decir, que se puede \"esquivar\" la matriz de $n \\times n$ usando matrices de $n \\times p$. También se puede usar, de forma equivalente,\n",
    "$$\n",
    "np.sum(A^T \\odot B, axis=0).T\n",
    "$$\n",
    "**queda a preferencia del alumno cuál usar.**\n",
    "\n",
    "## Desarrollo\n",
    "Para la demostración, se tomará cada lado de la igualdad para llevarla a una expresión tal que la equivalencia sea evidente.\n",
    "\n",
    "### Lado izquierdo\n",
    "Téngase la diagonal de una matriz resultante de un producto matricial, esto es,\n",
    "$$\n",
    "diag(A \\cdot B) = diag(C) = D\n",
    "$$ \n",
    "\n",
    "Por definición de diagonal principal, $C$ debe ser cuadrada. De allí, podemos decir que $C$ es de dimensión $n\\times n$. Además, para que el producto $A\\cdot B$ sea conforme, entonces $A$ es de $n\\times p$ y $B$ es de $p\\times n$. Por lo cual, sabiendo las dimensiones de la matriz que necesitamos para calcular su diagonal y cumpliendo con la conformidad del producto matricial, con seguridad decimos que $C_{n\\times n} = A_{n\\times p} \\cdot B_{p\\times n}$\n",
    "Ahora, $C_{n\\times n} = [c_{ij}]$, donde $i,j \\in \\{1,\\dots, n\\}$. Por definición de producto matricial, cada elemento está definido por\n",
    "$$\n",
    "c_{ij} = \\sum_{k=1}^p a_{ik} b_{kj}\n",
    "$$\n",
    "\n",
    "Si buscamos $diag(C)$, entonces solo estamos interesados en aquellos elementos de $C$ en donde $i=j$. Si definimos $D=diag(C)$, sabemos que $D$ es un vector en $R^n$, por lo que $D=[d_i]\\quad \\forall i \\in \\{1,\\dots,n\\}$. Con esto en mente, cada elemento de $D$ es\n",
    "\\begin{equation}\n",
    "d_i = c_{ii} = \\sum_{k=1}^p a_{ik} b_{ki}\n",
    "\\end{equation}\n",
    "\n",
    "### Lado derecho\n",
    "\n",
    "A partir del razonamiento anterior, sabemos que $A_{n\\times p} = [a_{ij}]$ y $B_{p\\times n} = [b_{ij}]$. Si transponemos $B$, entonces por definición $B^T_{n\\times p} = [b'_{ij}] = [b_{ji}]$.\n",
    "Definamos el [producto de Hadamard](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) como la multiplicación elemento a elemento entre dos matrices de iguales dimensiones, en este caso $n\\times p$. Así, $H_{n\\times p}=A\\odot B^T$, donde $H_{n\\times p}=[h_{ij}]$ tal que\n",
    "$$\n",
    "h_{ij​} = a_{ij​}\\cdot b_{ji}\n",
    "$$\n",
    "\n",
    "La expresión de la consigna exigía que $\\sum_{cols} A \\odot B^T$. En otras palabras, queremos sumar todos los $p$ elementos en cada i-ésima fila. Con ello obtenemos un vector $V \\in R^n$ que se compone de\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{2}\n",
    "v_i = \\sum^p_{j=1} h_{ij} = \\sum^p_{k=1} a_{ik}b'_{ik} = \\sum^p_{k=1} a_{ik}b_{ki}\n",
    "\\end{equation}\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "Del lado izquierdo, tenemos $D_{i} = [d_i] = diag(A\\cdot B)$; mientras que del derecho, $V_i = [v_i] = \\sum_{cols}  A\\odot B^T$ (donde $i \\in \\{1,\\dots,n\\}$). Entonces, por la ecuación 1 y 2 se sabe que los elementos de $D$ y de $V$ son equivalentes. En términos simbólicos, cada elemento resultante de ambos lados de la expresión está dado por\n",
    "$$\n",
    "v_i = d_i = \\sum^p_{k=1} a_{ik}b_{ki} \n",
    "$$\n",
    "Con esto, $diag(A \\cdot B) = \\sum_{cols} A \\odot B^T$ porque cada elemento del vector resultante posee los mismos elementos, demostrando lo requerido en la consigna.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from numpy import random as rnd"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Benchmarking:\n",
    "    \"\"\"\n",
    "    Una clase que usamos para realizar el benchmarking.\n",
    "\n",
    "    Al inicializar, genera dos matrices A y B de tamaño nxp y pxn,\n",
    "    respectivamente. Luego, puedo realizar el cálculo de forma ineficiente con\n",
    "    el método `.inefficient()`, o eficientemente con `.efficient()`\n",
    "\n",
    "    Argumentos:\n",
    "    - n: Entero positivo para las filas de A y las columnas de B.\n",
    "    - p: Entero positivo para las columnas de A y las filas de B.\n",
    "    - random_seed (opcional): Si le otorgo un valor, genero A y B con una\n",
    "    semilla predefinida.\n",
    "    \"\"\"\n",
    "    def __init__(self, n, p, random_seed=None):\n",
    "        if random_seed is not None:\n",
    "            rnd.seed(random_seed)\n",
    "        # Genero las matrices A y B con números aleatorios\n",
    "        self.A = rnd.rand(n,p)\n",
    "        self.B = rnd.rand(p,n)\n",
    "\n",
    "    def inefficient(self):\n",
    "        return np.diag(self.A @ self.B) # Lado izquierdo de la igualdad\n",
    "\n",
    "    def efficient(self):\n",
    "        return np.sum(self.A * self.B.T, axis=1) # Lado derecho de la igualdad"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para inicializar, utilizo números grandes a modo que la diferencia en eficiencia sea evidente: $n = 10.000$ y $p=10$. \n",
    "\n",
    "**Opcional:** Fijamos la semilla en 123 para asegurar reproducibilidad, pero podría omitirse o cambiarse en caso de querer experimentar con otros números.\n",
    "\n",
    "# Inicializo la clase\n",
    "calc = Benchmarking(n=10000, p=10, random_seed=123)\n",
    "\n",
    "Hago los cálculos, primero de forma ineficiente y luego de la manera eficiente.\n",
    "\n",
    "%%timeit\n",
    "calc.inefficient()\n",
    "\n",
    "%%timeit\n",
    "calc.efficient()\n",
    "\n",
    "En promedio, el método ineficiente tardó ~1 segundo por loop, mientras que el eficiente le tomó tan solo 650 microsegundos computar cada iteración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente en un nuevo modelo `EfficientQDA`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientQDA(FasterQDA):\n",
    "    \"\"\"\n",
    "    Vectorizamos la prediccion sobre TODAS las observaciones a la vez. \n",
    "    Esta versión arma una matriz (k, n, n) por clase y despues soquedamos \n",
    "    unicamente con la diagonal. \n",
    "    \"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: array de shape (p, n)  -> p features, n observaciones\n",
    "        return: array de shape (n,) con la clase predicha por observación\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Centramos todas las observaciones X con el vector de medias de CADA clase\n",
    "        #    - self.tensor_means tiene shape (k, p, 1)\n",
    "        #    - X tiene shape (p, n)\n",
    "        #    Con broadcasting, obtengo unbiased de shape (k, p, n): para cada clase k, \n",
    "        #    resto su media a todas las columnas de X.\n",
    "        unbiased = X[None, :, :] - self.tensor_means  # (k, p, n)\n",
    "\n",
    "        # 2) Aplicamos la forma cuadratica (x - μ)^T Σ^{-1} (x - μ):\n",
    "        #    Para EfficientQDA() evitamos crear la matriz intermedia M de (k, n, n) que \n",
    "        #    puede ser muy grande si el número de observaciones n es grande.\n",
    "        #    En su lugar nos quedamos con el producto Σ^{-1} (x - μ), representado \n",
    "        #    en la matriz A que se  muestra a continuación: \n",
    "        A = self.tensor_inv_cov @ unbiased  # (k, p, n)\n",
    "\n",
    "        # 3) Para la forma cuadratica solo necesitamps la diagonal por clase/observación:\n",
    "        #    Ahora se necesita calcular la forma cuadratica (x - μ)^T Σ^{-1} (x - μ) = U^T A\n",
    "        #    y obtener su diagonal aplicando la propiedad diag(A . B) = np.sum(A @ B.T, axis = 1)\n",
    "        #    que en este caso sería diag(U . A) = np.sum(U^T @ A, axis = 1)\n",
    "        #    Donde: U^T = (x - μ)^T con forma (k, n, p)\n",
    "        inner = np.sum(np.transpose(unbiased, (0, 2, 1)) @ A, axis=1)  # (k, n)\n",
    "\n",
    "        # 4) terminamos log-determinante para cada clase:\n",
    "        #    uso slogdet por estabilidad numérica.\n",
    "        #    Ojo: tengo Σ^{-1} ya precalculada; log|Σ^{-1}| = -log|Σ|.\n",
    "        #    slogdet sobre (k, p, p) me devuelve arrays de largo k.\n",
    "        _, logdet_inv = LA.slogdet(self.tensor_inv_cov)  # (k,)\n",
    "\n",
    "        # 5) armamos los log-condicionales por clase y por observacion:\n",
    "        #    broadcasting para que el logdet_inv (k,) se expanda a (k, n).\n",
    "        log_conditionals = 0.5 * logdet_inv[:, None] - 0.5 * inner  # (k, n)\n",
    "\n",
    "        # 6) sumamos log-priors por clase y elejimos la clase con mayor puntaje para cada obs\n",
    "        scores = self.log_a_priori[:, None] + log_conditionals  # (k, n)\n",
    "        yhat = np.argmax(scores, axis=0)  # (n,)\n",
    "        return yhat\n",
    "\n",
    "    # Dejo un helper opcional si quiero reusar la parte de log-condicionales en otros métodos\n",
    "    def _predict_log_conditionals_batch(self, X):\n",
    "        \"\"\"\n",
    "        Devolvemos la matriz (k, n) con los log-condicionales para todas las observaciones.\n",
    "        Solo lo usamos si queremos calcular proba/scores sin repetir codigo.\n",
    "        \"\"\"\n",
    "        unbiased = X[None, :, :] - self.tensor_means # (k, p, n)\n",
    "        A = self.tensor_inv_cov @ unbiased \n",
    "        inner = np.sum(np.transpose(unbiased, (0, 2, 1)) @ A, axis=1)\n",
    "        _, logdet_inv = LA.slogdet(self.tensor_inv_cov) # (k,)\n",
    "        return 0.5 * logdet_inv[:, None] - 0.5 * inner # (k, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 3) Diferencias entre implementaciones de QDA_Chol \n",
    "\n",
    "8. Si una matriz $A$ tiene fact. de Cholesky $A=LL^T$, expresar $A^{-1}$ en términos de $L$. ¿Cómo podría esto ser útil en la forma cuadrática de QDA?\n",
    "\n",
    "Si una matriz simetrica definida positiva $A$ tiene factorizacion de Cholesky $A = L L^\\top$ entonces su inversa se puede escribir como $A^{-1} = (L L^\\top)^{-1} = (L^\\top)^{-1} L^{-1}$\n",
    "\n",
    "En otras palabras, basta con invertir la matriz triangular inferior $L$ (y su traspuesta) en lugar de invertir $A$ directamente.\n",
    "\n",
    "En el contexto de QDA, la forma cuadrática que aparece es $(x - \\mu)^\\top \\, \\Sigma^{-1} \\, (x - \\mu)$\n",
    "\n",
    "Si calculamos la descomposicion de Cholesky $\\Sigma = L L^\\top$, podemos reescribir:\n",
    "\n",
    "$(x - \\mu)^\\top \\Sigma^{-1} (x - \\mu) = (x - \\mu)^\\top (L^\\top)^{-1} L^{-1} (x - \\mu)$\n",
    "\n",
    "Definiendo $z = L^{-1} (x - \\mu)$, esto se convierte en $z^\\top z = \\| z \\|^2$\n",
    "\n",
    "Esto es util porque evita calcular la inversa completa de $\\Sigma$ (que es muy caro para calcular). En cambio, solo necesitamos resolver sistemas triangulares con $L$, lo cual es mas eficiente y estable.\n",
    "\n",
    "\n",
    "\n",
    "9. Explicar las diferencias entre `QDA_Chol1`y `QDA` y cómo `QDA_Chol1` llega, paso a paso, hasta las predicciones.\n",
    "\n",
    "QDA_Chol1 tiene la misma estructura que QDA pero recurre a la factorización de Cholesky para resolver \n",
    "$$\\frac{1}{2}\\log |\\Sigma^{-1}| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$$ \n",
    "en forma más eficiente, de acuerdo con la factorización planteada en el punto anterior.\n",
    "\n",
    "Para ello, en lugar de calcular las matrices inversas de covarianza en `def _fit_params(self, X, y):` con\n",
    "\n",
    "`self.inv_covs = [LA.inv(np.cov(X[:,y.flatten()==idx], bias=True)) for idx in range(len(self.log_a_priori))]`\n",
    "\n",
    "QDA_Chol1 calcula las matrices inversas de Cholesky con:  \n",
    "\n",
    "`self.L_invs = [LA.inv(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True)) for idx in range(len(self.log_a_priori))`\n",
    "\n",
    "Separando en dos partes los cambios en `def _predict_log_conditional(self, x, class_idx):`, tenemos en primer lugar que el cálculo del determinante $$\n",
    "\\frac{1}{2}\\log |\\Sigma^{-1}|$$ cambia de `np.log(LA.det(inv_cov))` a `np.log(L_inv.diagonal().prod())`, donde `L_inv` es la inversa de la matriz de Cholesky para cada categoría. En segundo lugar, el producto $$- \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)\n",
    "$$ pasa de ser `-0.5 * unbiased_x.T @ inv_cov @ unbiased_x`a ser `-0.5 * (y**2).sum()` con `y = L_inv @ unbiased_x`. Las operaciones en QDA y QDA_Col1 son algebráicamente equivalentes pero computacionalmente diferentes (QDA_Chol1 es más barata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDA_Chol1(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.L_invs = [\n",
    "        LA.inv(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True))\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "    # Este paso estima la matriz de covarianza sigma para cada categoría idx, \n",
    "    # calcula la matriz de Cholesky correspondiente y calcula su inversa L^(-1)\n",
    "    # y las acumula en L_invs.\n",
    "    # lower=True devuelve la matriz triangular inferior\n",
    "    # bias=True calcula la covarianza por máxima verosimilitud (divide por n en lugar de n-1)\n",
    " \n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "    # Este paso calcula means de la misma forma que en QDA\n",
    "    \n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L_inv = self.L_invs[class_idx]\n",
    "    # L_inv toma la inversa de la matriz de Cholesky de una clase class_idx\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "    # unbiased_x calcula (x-mu_j)\n",
    "\n",
    "    y = L_inv @ unbiased_x\n",
    "    # calcula L(-1) @ (x-mu) en la factorización de Cholesky de \n",
    "    # (x-mu).T @ sigma^(-1) @ (x-mu) = (x-mu).T @ (L.T)^(-1) @ L^(-1) @ (x-mu), \n",
    "    # recordando que (L^(-1)).T = (L.T)^(-1) si L es invertible\n",
    "\n",
    "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()\n",
    "    # la primera parte de la resta calcula el log del determinante sabiendo que\n",
    "    # el determinante de una matriz triangular es el producto de los valores de su diagonal\n",
    "    # El det(sigma^(-1)) = det(L^(-1))^2, por lo que 0.5 * log(det(sigma^(-1))) = log(det(L^(-1))) = np.log(L_inv.diagonal().prod())\n",
    "    # por lo que esté término es equivalente para QDA y QDA_Chol1\n",
    "    # la segunda parte calcula 0.5 * norma de y, sabiendo por los desarrollos en el punto 8 que\n",
    "    # 0.5 * (y**2) = 0.5 * (x-mu).T @ sigma^(-1) @ (x-mu), que es la operación que realiza QDA\n",
    "    # tomado ambos términos de la resta, concluimos que las operaciones de QDA y QDA_Chol1 son algebráicamnte equivalentes"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "10. ¿Cuáles son las diferencias entre `QDA_Chol1`, `QDA_Chol2` y `QDA_Chol3`?\n",
    "\n",
    "Las clases `QDA_Chol*` implementan, como su nombre lo indica, un algoritmo para aplicar Análisis Discriminante Cuadrático (QDA). Su diferencia está en cómo cada una realiza esta tarea. Todas se basan en la descomposición de Cholesky de la matriz de covarianianza ($\\sigma$), que consiste en encontrar una matriz triangular inferior única $L$ tal que $\\Sigma = L\\cdot L^T$.\n",
    "\n",
    "### QDA_Chol1\n",
    "Esta es la implementación más *naive* de todas. Su punto a favor es que resulta fácil de escribir y entender. En contra, su ineficiencia computacional.\n",
    "\n",
    "#### _fit_params\n",
    "\n",
    "Comenzamos con el ajuste. Tiene dos atributos\n",
    "\n",
    "- **Inversa:** Primero, se calcula la matriz de covarianza $\\Sigma_k$​ para los datos de la clase actual, para luego aplicarle la descomposición de Cholesky sobre $\\Sigma_k$ tal que $\\Sigma_k = L_k L_k^T$. A esto, simplemente se le aplica su inversa (lo que computacionalmente suele ser relativamente demandante).\n",
    "\n",
    "- **Vector de medias** El código itera sobre cada clase. Para cada una de ellas, selecciona de la matriz de datos $X$ solo las columnas que corresponden a esa clase. A esto, le toma la media (`axis=1` porque lo hace a través de las columnas).\n",
    "\n",
    "#### _predict_log_conditional\n",
    "\n",
    "A partir de `_fit_params`, obtiene vector de medias $\\mu_k$ y la inversa de Cholesky $L_k^{-1}$ para calcular $y=L_k^{-1}\\cdot (x-\\mu_k)$. El último paso es aplicar la fórmula para la log probabilidad Normal multivariante.\n",
    "\n",
    "### QDA_Chol2\n",
    "\n",
    "Acá nos evitamos directamente el cálculo de la inversa y resolvemos el sistema lineal directamente usando `solve_triangular`.\n",
    "\n",
    "#### _fit_params\n",
    "\n",
    "Comenzamos con el ajuste. Tiene dos atributos:\n",
    "\n",
    "- **Inversa:** Calcula la matriz de covarianza $\\Sigma_k$​ para la clase actual para luego aplicar Cholesky sobre $\\Sigma_k$ para obtener la matriz triangular inferior $L_k$.\n",
    "\n",
    "- **Vector de medias** Igual a QDA_Chol1.\n",
    "\n",
    "#### _predict_log_conditional\n",
    "\n",
    "A partir de que la ecuación $y=L_k^ {−1}​(x−\\mu_k​)$ es matemáticamente idéntica a resolver el sistema de ecuaciones lineales $L_k​y=(x−\\mu_k​)$ para la incógnita $y$, se resuelve el sistema con la función solve_triangular para luego calcular la log-probabilidad, con la diferencia de que ahora se toma el logaritmo del producto de la diagonal de la matriz L, resultando algo más directo que QDA_Chol1. Al evitar el cálculo de la inversa, esta implementación resulta la **más eficiente computacionalmente**.\n",
    "\n",
    "### QDA_Chol3\n",
    "\n",
    "Este es un punto intermedio, que no evita calcular la inversa, pero lo hace de un modo ligeramente más eficiente que QDA_Chol1.\n",
    "\n",
    "#### _fit_params\n",
    "\n",
    "Comenzamos con el ajuste. Tiene dos atributos:\n",
    "\n",
    "- **Inversa:** Calcula la matriz de covarianza $\\Sigma_k$​ para la clase actual\n",
    "para luego aplicar Cholesky sobre $\\Sigma_k$ para obtener la matriz triangular\n",
    "inferior $L_k$. Aquí, en lugar de usar la función genérica LA.inv(), utiliza\n",
    "dtrtri. Esta es una función de bajo nivel proveniente de la biblioteca LAPACK.\n",
    "Significa \"double precision triangular inverse\", y está optimizada para invertir\n",
    "matrices triangulares.\n",
    "\n",
    "- **Vector de medias** Igual a QDA_Chol1.\n",
    "\n",
    "#### _predict_log_conditional\n",
    "\n",
    "Idéntico a QDA_Chol1, simplemente porque ambos métodos trabajan con la inversa."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### 4) Optimización\n",
    "\n",
    "13. Implementar el modelo `EfficientChol` combinando los insights de `EfficientQDA` y `TensorizedChol`. Si se desea, se puede implementar `FasterChol` como ayuda, pero no se contempla para el punto.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class EfficientChol(BaseBayesianClassifier):\n",
    "    \"\"\"\n",
    "    Version mas eficiente con Cholesky + vectorización por clases y observaciones.\n",
    "    - En fit: para cada clase j, hacemos Cholesky Sigma_j = L_j L_j^T y precomputamos L_j^{-1}.\n",
    "    - En predict: para todas las observaciones a la vez, calculamos y = L_j^{-1} (x - mu_j) y usamos ||y||^2.\n",
    "    \"\"\"\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        k = len(np.unique(y))\n",
    "        # medias por clase (k, p, 1)\n",
    "        self.means = [X[:, y.flatten() == idx].mean(axis=1, keepdims=True)\n",
    "                      for idx in range(k)]\n",
    "        self.tensor_means = np.stack(self.means)  # (k, p, 1)\n",
    "\n",
    "        # Cholesky por clase y su inversa triangular inferior (k, p, p)\n",
    "        # Usamos dtrtri (como en QDA_Chol3) para obtener L^{-1} de forma estable y rapida\n",
    "        l_invs = []\n",
    "        logdet_inv = []\n",
    "        for idx in range(k):\n",
    "            cov_j = np.cov(X[:, y.flatten() == idx], bias=True)\n",
    "            L_j = cholesky(cov_j, lower=True)\n",
    "            L_inv_j = dtrtri(L_j, lower=1)[0] # triangular inferior invertida\n",
    "            l_invs.append(L_inv_j)\n",
    "\n",
    "            # log|Sigma_j^{-1}| = -log|Sigma_j| = -2 * sum(log(diag(L_j)))\n",
    "            # Equivalente:  +2 * sum(log(diag(L_inv_j))) (usamos esta porque ya tenemos L_inv)\n",
    "            logdet_inv.append( 2.0 * np.log(np.diag(L_inv_j)).sum() )\n",
    "\n",
    "        self.tensor_L_inv = np.stack(l_invs) # (k, p, p)\n",
    "        self.logdet_inv = np.asarray(logdet_inv) # (k,)\n",
    "\n",
    "    def _predict_log_conditionals_batch(self, X):\n",
    "        \"\"\"\n",
    "        Devuelve (k, n): log p(x | clase=j) para todas las observaciones a la vez.\n",
    "        y = L_j^{-1} (x - mu_j)  =>  (x-mu)^T Sigma^{-1} (x-mu) = ||y||^2\n",
    "        \"\"\"\n",
    "        # X: (p, n)\n",
    "        unbiased = X[None, :, :] - self.tensor_means # (k, p, n)\n",
    "        Y = self.tensor_L_inv @ unbiased # (k, p, n)\n",
    "        quad = np.sum(Y * Y, axis=1) # (k, n)  = ||y||^2 por clase/obs\n",
    "\n",
    "        # 0.5*log|Sigma^{-1}| - 0.5*quad\n",
    "        return 0.5 * self.logdet_inv[:, None] - 0.5 * quad\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: (p, n)\n",
    "        return: (n,)\n",
    "        \"\"\"\n",
    "        log_cond = self._predict_log_conditionals_batch(X) # (k, n)\n",
    "        scores = self.log_a_priori[:, None] + log_cond # (k, n)\n",
    "        return np.argmax(scores, axis=0) # (n,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completados: 1, 2, 3, 4, 8 \n",
    "\n",
    "### Maximiliano Christener: 5 - 10\n",
    "### Luiz Diaz: 6 - 11\n",
    "### Mariano Fagre: 7 - 12\n",
    "### Juan Pablo Skobalski: 4 - 8 - 13\n",
    "### Ronald Uthurralt: 9 - 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c4640fad3de46af8ee288fb94e9831d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cead8148ae5469eb637c29de2b21ec7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "16f5ac0cf3cc441d891ae9522cf0798c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e4f83990b0c4572952d5e23005283fb",
      "placeholder": "​",
      "style": "IPY_MODEL_b85cd793196847cda92178f945624016",
      "value": "TensorizedQDA (MEM): 100%"
     }
    },
    "1d3c4fad476c47208bce6625ec783f6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c4640fad3de46af8ee288fb94e9831d",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_70c3a115637d4880802a605974661714",
      "value": 20
     }
    },
    "1da7d96834cc4bc8a3cd5595b3446b74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de9f39ade1b04c22921589c687b25518",
      "placeholder": "​",
      "style": "IPY_MODEL_437e4360cf5e44bc845c2f6002ea2d55",
      "value": " 100/100 [00:02&lt;00:00, 40.27it/s]"
     }
    },
    "254e2937d92d48b0886e5d950a6bbefa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32df4fc69a3c47cf9a220f4469b667a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "373a7a1549b44c2696083efddc88fafe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a99fe4554914c12ad9146aea349708c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_254e2937d92d48b0886e5d950a6bbefa",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0cead8148ae5469eb637c29de2b21ec7",
      "value": 100
     }
    },
    "3f110d0a165e41b0a268f77b2d2a11b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "437e4360cf5e44bc845c2f6002ea2d55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ab824715b294c86b045c7e187125524": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "514491be27294e0cb5b70b6e25ee3355": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54a06911978746dd9e81bb1526bd741c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e4f83990b0c4572952d5e23005283fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6197d697782b4688a278663e287317b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "684de1d1bee34b309988bc0f40b0d5e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3fe060bd70848b7af130d99bbff6839",
      "placeholder": "​",
      "style": "IPY_MODEL_32df4fc69a3c47cf9a220f4469b667a3",
      "value": "QDA (TIME): 100%"
     }
    },
    "6d70e647d03f4004bd727673fbbfe5f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70c3a115637d4880802a605974661714": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "74c98c4598bd438cbedbb6c8a492eb15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "766927c256e3409ab53c04ca092964e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f722aef90b84352b16d45f8716df168": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8aeeac18744940f8b4940256613178db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_16f5ac0cf3cc441d891ae9522cf0798c",
       "IPY_MODEL_bc84ff51ad3c4921b7aca214f0ff5e35",
       "IPY_MODEL_f8508a02a66547b3a501694a1b8633ab"
      ],
      "layout": "IPY_MODEL_4ab824715b294c86b045c7e187125524"
     }
    },
    "8df46d45d37e45e889dbec220fa360bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f722aef90b84352b16d45f8716df168",
      "placeholder": "​",
      "style": "IPY_MODEL_f356ef11b0734ce6a31ec9f0a01e055a",
      "value": " 100/100 [00:01&lt;00:00, 57.18it/s]"
     }
    },
    "8e2c2ff9e34b49568449aa7eb991d5cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a2e9076aca0b4224ba3aa8e4c3239260": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6197d697782b4688a278663e287317b1",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c69d75dc1d1b4fca925f308a79a7e248",
      "value": 100
     }
    },
    "a33f440ec8454a2db5bd26485d71c4db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f110d0a165e41b0a268f77b2d2a11b7",
      "placeholder": "​",
      "style": "IPY_MODEL_de398ad23ecd44d8b8d69e621401f3b3",
      "value": " 20/20 [00:02&lt;00:00,  7.34it/s]"
     }
    },
    "a806c90e849243999c2fce804e20b449": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1c06f1940204bdaacf4ad3d843faa2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b85cd793196847cda92178f945624016": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc84ff51ad3c4921b7aca214f0ff5e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1c06f1940204bdaacf4ad3d843faa2f",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_74c98c4598bd438cbedbb6c8a492eb15",
      "value": 20
     }
    },
    "c01ec4015f1c4c4491687244977cf5ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f07c3a390d8640e586ec729d3c7c86d8",
       "IPY_MODEL_1d3c4fad476c47208bce6625ec783f6d",
       "IPY_MODEL_a33f440ec8454a2db5bd26485d71c4db"
      ],
      "layout": "IPY_MODEL_766927c256e3409ab53c04ca092964e0"
     }
    },
    "c29e6bbacf9c45ecb5465624ee315b70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d70e647d03f4004bd727673fbbfe5f9",
      "placeholder": "​",
      "style": "IPY_MODEL_c3149d7e47c84328904326dda8527af4",
      "value": "TensorizedQDA (TIME): 100%"
     }
    },
    "c3149d7e47c84328904326dda8527af4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c69d75dc1d1b4fca925f308a79a7e248": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d0c5a2b6b27144268b7fc0bedcc33a86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_684de1d1bee34b309988bc0f40b0d5e2",
       "IPY_MODEL_3a99fe4554914c12ad9146aea349708c",
       "IPY_MODEL_1da7d96834cc4bc8a3cd5595b3446b74"
      ],
      "layout": "IPY_MODEL_d21ef446c090431480050c3352e2634e"
     }
    },
    "d21ef446c090431480050c3352e2634e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3fe060bd70848b7af130d99bbff6839": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de1229493a5749c799bcece8c07506f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c29e6bbacf9c45ecb5465624ee315b70",
       "IPY_MODEL_a2e9076aca0b4224ba3aa8e4c3239260",
       "IPY_MODEL_8df46d45d37e45e889dbec220fa360bd"
      ],
      "layout": "IPY_MODEL_514491be27294e0cb5b70b6e25ee3355"
     }
    },
    "de398ad23ecd44d8b8d69e621401f3b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de9f39ade1b04c22921589c687b25518": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f07c3a390d8640e586ec729d3c7c86d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54a06911978746dd9e81bb1526bd741c",
      "placeholder": "​",
      "style": "IPY_MODEL_a806c90e849243999c2fce804e20b449",
      "value": "QDA (MEM): 100%"
     }
    },
    "f356ef11b0734ce6a31ec9f0a01e055a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8508a02a66547b3a501694a1b8633ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_373a7a1549b44c2696083efddc88fafe",
      "placeholder": "​",
      "style": "IPY_MODEL_8e2c2ff9e34b49568449aa7eb991d5cb",
      "value": " 20/20 [00:00&lt;00:00, 24.64it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
